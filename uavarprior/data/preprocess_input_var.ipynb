{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b3ec0ca",
   "metadata": {},
   "source": [
    "# Preprocessing Input Variants\n",
    "\n",
    "This notebook processes variant data from multiple sources (ClinVar, COSMIC, and MAF) to create a unified dataset for downstream analysis in the UAVarPrior project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4326c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "print('Packages loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c143f",
   "metadata": {},
   "source": [
    "## Load Dataset Files\n",
    "\n",
    "First, we'll load the variant data from multiple sources:\n",
    "1. ClinVar - Clinically relevant variants\n",
    "2. COSMIC - Catalogue of Somatic Mutations in Cancer\n",
    "3. MAF - Mutation Annotation Format (typically from TCGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ad8b973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ClinVar data...\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 6, saw 4\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load ClinVar data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading ClinVar data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m cln_var_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclinvar_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cln_var_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ClinVar variants\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(cln_var_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/envs/fugep/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/fugep/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/fugep/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/envs/fugep/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 6, saw 4\n"
     ]
    }
   ],
   "source": [
    "# Define paths to input data files\n",
    "# You may need to adjust these paths to match your environment\n",
    "data_dir = '/scratch/ml-csm/projects/fgenom/gve/data/'\n",
    "clinvar_path = os.path.join(data_dir, 'clinVar/clinvar_20250202.vcf.gz')\n",
    "cosmic_path = os.path.join(data_dir, 'COSMIC/COSMIC_noncoding_var.parquet.gzip')\n",
    "maf_path = '/scratch/ml-csm/datasets/genomics/ref-genome/human/GRCh38/ensembl/variants/processed/'\n",
    "maf_path = os.path.join(maf_path, '1000GENOMES-release114-maf.parquet.gz')\n",
    "\n",
    "\n",
    "# Load ClinVar data\n",
    "print(\"Loading ClinVar data...\")\n",
    "cln_var_df = pd.read_csv(clinvar_path)\n",
    "print(f\"Loaded {len(cln_var_df)} ClinVar variants\")\n",
    "print(cln_var_df.head())\n",
    "print(\"\\nClinVar columns:\", cln_var_df.columns.tolist())\n",
    "\n",
    "# Load COSMIC data\n",
    "print(\"\\nLoading COSMIC data...\")\n",
    "cosmic_df = pd.read_csv(cosmic_path)\n",
    "print(f\"Loaded {len(cosmic_df)} COSMIC variants\")\n",
    "print(cosmic_df.head())\n",
    "print(\"\\nCOSMIC columns:\", cosmic_df.columns.tolist())\n",
    "\n",
    "# Load MAF data\n",
    "print(\"\\nLoading MAF data...\")\n",
    "df_maf = pd.read_csv(maf_path)\n",
    "print(f\"Loaded {len(df_maf)} MAF variants\")\n",
    "print(df_maf.head())\n",
    "print(\"\\nMAF columns:\", df_maf.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f13af8",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Each dataset may have different column names and formats. We'll examine the data and standardize the chromosome and position information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in key columns\n",
    "print(\"Missing values in key columns:\")\n",
    "print(\"\\nClinVar:\")\n",
    "print(cln_var_df[['chrom', 'pos']].isna().sum())\n",
    "\n",
    "print(\"\\nCOSMIC:\")\n",
    "print(cosmic_df[['chrom', 'pos']].isna().sum())\n",
    "\n",
    "print(\"\\nMAF:\")\n",
    "print(df_maf[['chrom', 'pos']].isna().sum())\n",
    "\n",
    "# Drop rows with missing chromosome or position information\n",
    "cln_var_df = cln_var_df.dropna(subset=['chrom', 'pos'])\n",
    "cosmic_df = cosmic_df.dropna(subset=['chrom', 'pos'])\n",
    "df_maf = df_maf.dropna(subset=['chrom', 'pos'])\n",
    "\n",
    "print(\"\\nAfter dropping missing values:\")\n",
    "print(f\"ClinVar: {len(cln_var_df)} variants\")\n",
    "print(f\"COSMIC: {len(cosmic_df)} variants\")\n",
    "print(f\"MAF: {len(df_maf)} variants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db87de",
   "metadata": {},
   "source": [
    "## Standardize Chromosome Format\n",
    "\n",
    "Ensure chromosome names are consistent across datasets (e.g., with or without \"chr\" prefix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize chromosome format\n",
    "def standardize_chrom(chrom):\n",
    "    \"\"\"\n",
    "    Standardize chromosome format to ensure consistency.\n",
    "    Convert to string, remove 'chr' prefix if present, and handle special cases.\n",
    "    \"\"\"\n",
    "    chrom = str(chrom)\n",
    "    # Remove 'chr' prefix if present\n",
    "    if chrom.startswith('chr'):\n",
    "        chrom = chrom[3:]\n",
    "    # Handle mitochondrial chromosome\n",
    "    if chrom.lower() in ['m', 'mt']:\n",
    "        chrom = 'MT'\n",
    "    return chrom\n",
    "\n",
    "# Apply standardization to chromosome columns\n",
    "cln_var_df['chrom'] = cln_var_df['chrom'].apply(standardize_chrom)\n",
    "cosmic_df['chrom'] = cosmic_df['chrom'].apply(standardize_chrom)\n",
    "df_maf['chrom'] = df_maf['chrom'].apply(standardize_chrom)\n",
    "\n",
    "# Check unique chromosome values to ensure consistency\n",
    "print(\"Unique chromosome values after standardization:\")\n",
    "print(\"\\nClinVar:\", sorted(cln_var_df['chrom'].unique()))\n",
    "print(\"\\nCOSMIC:\", sorted(cosmic_df['chrom'].unique()))\n",
    "print(\"\\nMAF:\", sorted(df_maf['chrom'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ee28a",
   "metadata": {},
   "source": [
    "## Ensure Position is Integer Type\n",
    "\n",
    "Make sure all position values are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb10eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert position to integer type\n",
    "cln_var_df['pos'] = cln_var_df['pos'].astype(int)\n",
    "cosmic_df['pos'] = cosmic_df['pos'].astype(int)\n",
    "df_maf['pos'] = df_maf['pos'].astype(int)\n",
    "\n",
    "# Verify data types\n",
    "print(\"Data types after conversion:\")\n",
    "print(\"\\nClinVar:\")\n",
    "print(cln_var_df[['chrom', 'pos']].dtypes)\n",
    "print(\"\\nCOSMIC:\")\n",
    "print(cosmic_df[['chrom', 'pos']].dtypes)\n",
    "print(\"\\nMAF:\")\n",
    "print(df_maf[['chrom', 'pos']].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aea980",
   "metadata": {},
   "source": [
    "## Combine Dataframes\n",
    "\n",
    "Combine the three datasets, keeping only the chromosome and position columns, and remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f389575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract Relevant Columns\n",
    "# Extract only chrom and pos columns from each dataframe\n",
    "cln_var_subset = cln_var_df[['chrom', 'pos']].copy()\n",
    "cosmic_subset = cosmic_df[['chrom', 'pos']].copy()\n",
    "df_maf_subset = df_maf[['chrom', 'pos']].copy()\n",
    "\n",
    "# Step 2: Concatenate the Dataframes\n",
    "# Combine all three dataframes\n",
    "combined_df = pd.concat([cln_var_subset, cosmic_subset, df_maf_subset], axis=0)\n",
    "\n",
    "# Step 3: Remove Duplicates\n",
    "# Remove any duplicate rows (positions that appear in multiple datasets)\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "# Step 4: Reset Index\n",
    "# Reset the index to have a clean sequential index\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "# Step 5: Verify and Standardize Data Types\n",
    "# Ensure consistent data types across the combined dataframe\n",
    "combined_df['chrom'] = combined_df['chrom'].astype(str)\n",
    "combined_df['pos'] = combined_df['pos'].astype(int)\n",
    "\n",
    "# Print summary information\n",
    "print(f\"Total variants after combining and removing duplicates: {len(combined_df)}\")\n",
    "print(\"\\nSample of combined dataframe:\")\n",
    "print(combined_df.head())\n",
    "\n",
    "# Print counts by source (before deduplication)\n",
    "print(f\"\\nSource counts (before deduplication):\")\n",
    "print(f\"ClinVar variants: {len(cln_var_subset)}\")\n",
    "print(f\"COSMIC variants: {len(cosmic_subset)}\")\n",
    "print(f\"MAF variants: {len(df_maf_subset)}\")\n",
    "print(f\"Total combined (after deduplication): {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c4283b",
   "metadata": {},
   "source": [
    "## Distribution Analysis\n",
    "\n",
    "Analyze the distribution of variants across chromosomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count variants by chromosome\n",
    "chrom_counts = combined_df['chrom'].value_counts().sort_index()\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.barplot(x=chrom_counts.index, y=chrom_counts.values)\n",
    "plt.title('Variant Distribution by Chromosome')\n",
    "plt.xlabel('Chromosome')\n",
    "plt.ylabel('Number of Variants')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate percentage distribution\n",
    "percentage_dist = (chrom_counts / chrom_counts.sum() * 100).round(2)\n",
    "print(\"Percentage distribution by chromosome:\")\n",
    "for chrom, percentage in percentage_dist.items():\n",
    "    print(f\"Chromosome {chrom}: {percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c0d6d",
   "metadata": {},
   "source": [
    "## Add Dummy Label Column\n",
    "\n",
    "For compatibility with downstream analysis tools, add a dummy \"label\" column with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a label column (required by some downstream tools)\n",
    "combined_df['label'] = 0\n",
    "\n",
    "print(\"Added 'label' column with zeros:\")\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4a052",
   "metadata": {},
   "source": [
    "## Save the Processed Data\n",
    "\n",
    "Save the combined and processed dataframe to a parquet file for efficient storage and access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs directory if it doesn't exist\n",
    "output_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'outputs')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Saving large files to {output_dir} (not tracked by Git)\")\n",
    "\n",
    "# Save to parquet file with compression\n",
    "output_path = os.path.join(output_dir, 'combined_variant_positions.parquet.gz')\n",
    "combined_df.to_parquet(output_path, compression='gzip')\n",
    "print(f\"Saved combined variant positions to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8a04b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "1. Loaded variant data from three sources (ClinVar, COSMIC, MAF)\n",
    "2. Standardized chromosome and position information\n",
    "3. Combined the datasets while removing duplicates\n",
    "4. Added a dummy label column\n",
    "5. Saved the processed data to a compressed parquet file\n",
    "\n",
    "The resulting dataset can be used for downstream analysis in the UAVarPrior project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b17f71aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "print('Packages loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f36bd94-333e-48a3-ac4a-13758ec5e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vcf(vcf_file_path):\n",
    "    \"\"\"\n",
    "    Reads data from vcf file and parse lines into pandas dataframe\n",
    "        Parameters:\n",
    "            vcf_file_path: str\n",
    "        \n",
    "        Returns:\n",
    "            pandas dataframe with columns [\"CHROM\", \"POS\", \"ID\", \"REF\", \"ALT\", \"QUAL\", \"FILTER\", \"INFO\"]\n",
    "    \"\"\"\n",
    "    VCF_REQUIRED_COLS = [\"#CHROM\", \"POS\", \"ID\", \"REF\", \"ALT\"]\n",
    "    with gzip.open(vcf_file_path, 'rb') as file_handle:\n",
    "        lines = file_handle.readlines()\n",
    "        \n",
    "        # handling first few lines\n",
    "        index = 0\n",
    "        for index, line in enumerate(lines):\n",
    "            line = line.decode('utf8')\n",
    "            if '#' not in line:\n",
    "                break\n",
    "            if \"#CHROM\" in line:\n",
    "                cols = line.strip().split('\\t')\n",
    "                if cols[:5] != VCF_REQUIRED_COLS:\n",
    "                    raise ValueError(\n",
    "                        \"First 5 columns in file {0} were {1}. \"\n",
    "                        \"Expected columns: {2}\".format(\n",
    "                            input_path, cols[:5], VCF_REQUIRED_COLS))\n",
    "                index += 1\n",
    "                break  \n",
    "         \n",
    "    # handling remaining lines of vcf file\n",
    "    variants = []\n",
    "    for line in lines[index:]:\n",
    "        line = line.decode('utf8')\n",
    "        cols = line.strip().split('\\t')\n",
    "        variants.append(cols)\n",
    "\n",
    "    df = pd.DataFrame(variants)\n",
    "    print(df.shape)\n",
    "    df.columns = [\"CHROM\", \"POS\", \"ID\", \"REF\", \"ALT\", \"QUAL\", \"FILTER\", \"INFO\"]\n",
    "    return df\n",
    "\n",
    "def remove_extra_chroms(df):\n",
    "    chroms = [str(chrom) for chrom in range(1, 23)]\n",
    "    df_var = pd.DataFrame()\n",
    "    for chrom in chroms:\n",
    "        chr_df = df[df['CHROM']==str(chrom)]\n",
    "#         print(chrom, chr_df.shape)\n",
    "        df_var = pd.concat([df_var, chr_df])\n",
    "    return df_var\n",
    "\n",
    "\n",
    "def get_unique_id_dict(df_var):\n",
    "    \"\"\"\n",
    "    Extracts unique ID types from info column of vcf data\n",
    "        Parameters:\n",
    "            df_var: pandas dataframe with info as one of the columns\n",
    "        \n",
    "        Returns:\n",
    "            dictionary of unique ids with None value as default\n",
    "    \"\"\"\n",
    "    max_info = 0\n",
    "    unique_ids = []\n",
    "    for info_row in df_var['INFO'].values:\n",
    "        splits = info_row.split(';')\n",
    "        slen = len(splits)\n",
    "        if max_info <= slen:\n",
    "            max_info = slen\n",
    "            for splt in splits:\n",
    "                unique_ids.append(splt.split('=')[0])\n",
    "\n",
    "    unique_ids = np.unique(np.array(unique_ids))\n",
    "    uniq_id_dict = {'DBVARID': None} # Somehow this key couldn't be extracted by above code\n",
    "    for uid in unique_ids:\n",
    "        uniq_id_dict[uid] = None\n",
    "    return uniq_id_dict\n",
    "\n",
    "def info_row_expand(info_row, uniq_id_dict):\n",
    "    \"\"\"\n",
    "    Expands info row by splitting it into different ids\n",
    "        Parameters:\n",
    "            info_row (str): info text row\n",
    "            uniq_id_dict (dictionary): dictionary with unique ids\n",
    "            \n",
    "        Returns:\n",
    "            dictionary with unique ids as keys and values extracted from info as values\n",
    "    \"\"\"\n",
    "    temp_dict = {}\n",
    "    for key, value in uniq_id_dict.items():\n",
    "        temp_dict[key] = value\n",
    "    keys = uniq_id_dict.keys()\n",
    "        \n",
    "    splits = info_row.split(';')\n",
    "    for spl in splits:\n",
    "        temp = spl.split('=')\n",
    "        if temp[0] in keys:\n",
    "            temp_dict[temp[0]] = temp[1]\n",
    "        else:\n",
    "            print(f'Key: {temp[0]} value: {temp[1]} are extra items')\n",
    "    return np.array(list(temp_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33724660-4345-44c7-9dc5-5e85af8852dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3221834, 8)\n",
      "35\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>CLNDN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>66926</td>\n",
       "      <td>Retinitis_pigmentosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>69134</td>\n",
       "      <td>not_specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>69314</td>\n",
       "      <td>not_specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>69423</td>\n",
       "      <td>not_specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>69581</td>\n",
       "      <td>not_specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099325</th>\n",
       "      <td>22</td>\n",
       "      <td>50777972</td>\n",
       "      <td>not_specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099326</th>\n",
       "      <td>22</td>\n",
       "      <td>50777976</td>\n",
       "      <td>not_specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099327</th>\n",
       "      <td>22</td>\n",
       "      <td>50782204</td>\n",
       "      <td>not_specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099328</th>\n",
       "      <td>22</td>\n",
       "      <td>50782243</td>\n",
       "      <td>not_specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099329</th>\n",
       "      <td>22</td>\n",
       "      <td>50782258</td>\n",
       "      <td>not_specified</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3099330 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CHROM       POS                 CLNDN\n",
       "0           1     66926  Retinitis_pigmentosa\n",
       "1           1     69134         not_specified\n",
       "2           1     69314         not_specified\n",
       "3           1     69423         not_specified\n",
       "4           1     69581         not_specified\n",
       "...       ...       ...                   ...\n",
       "3099325    22  50777972         not_specified\n",
       "3099326    22  50777976         not_specified\n",
       "3099327    22  50782204         not_specified\n",
       "3099328    22  50782243         not_specified\n",
       "3099329    22  50782258         not_specified\n",
       "\n",
       "[3099330 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clin_var_file = '/scratch/ml-csm/projects/fgenom/gve/data/human/clinVar/clinvar_20250202.vcf.gz'\n",
    "df = read_vcf(clin_var_file)\n",
    "\n",
    "\n",
    "df = remove_extra_chroms(df)\n",
    "\n",
    "uniq_id_dict = get_unique_id_dict(df)\n",
    "print(len(uniq_id_dict))\n",
    "\n",
    "\n",
    "values = []\n",
    "for info_row in df['INFO'].values:\n",
    "    row_values = info_row_expand(info_row, uniq_id_dict)\n",
    "    values.append(row_values)\n",
    "    \n",
    "values = np.array(values)\n",
    "cln_var_df = pd.DataFrame(values)\n",
    "cln_var_df.columns = list(uniq_id_dict.keys())\n",
    "cln_var_df = pd.concat([df.iloc[:, [0, 1]], cln_var_df.loc[:, ['CLNDN']]], axis=1)\n",
    "# cln_var_df = pd.concat([df.iloc[:, [0, 1]], cln_var_df], axis=1)\n",
    "del df\n",
    "del values\n",
    "cln_var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf9e393c-2619-441e-a51a-65cc294ef939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3099330 entries, 0 to 3099329\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   chrom   int8  \n",
      " 1   pos     uint32\n",
      " 2   CLNDN   object\n",
      "dtypes: int8(1), object(1), uint32(1)\n",
      "memory usage: 62.1+ MB\n"
     ]
    }
   ],
   "source": [
    "cln_var_df.columns = ['chrom', 'pos', 'CLNDN']\n",
    "cln_var_df['chrom'] = cln_var_df['chrom'].astype(np.int8)\n",
    "cln_var_df['pos'] = cln_var_df['pos'].astype(np.uint32)\n",
    "cln_var_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9d679af-6f97-40c2-8b7f-9cbad9b0bf7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16727025</th>\n",
       "      <td>9</td>\n",
       "      <td>138258488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16727026</th>\n",
       "      <td>9</td>\n",
       "      <td>138258493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16727027</th>\n",
       "      <td>9</td>\n",
       "      <td>138258852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16727028</th>\n",
       "      <td>9</td>\n",
       "      <td>138258875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16727029</th>\n",
       "      <td>9</td>\n",
       "      <td>138259287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16727030 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          chrom        pos\n",
       "0             1      10108\n",
       "1             1      10108\n",
       "2             1      10108\n",
       "3             1      10151\n",
       "4             1      10151\n",
       "...         ...        ...\n",
       "16727025      9  138258488\n",
       "16727026      9  138258493\n",
       "16727027      9  138258852\n",
       "16727028      9  138258875\n",
       "16727029      9  138259287\n",
       "\n",
       "[16727030 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load COSMIC noncoding variant data and convert column types\n",
    "cosmic_df = pd.read_parquet(\n",
    "    '/scratch/ml-csm/projects/fgenom/gve/data/human/COSMIC/COSMIC_noncoding_var.parquet.gzip',\n",
    "    columns=['chrom', 'pos']\n",
    ")\n",
    "cosmic_df['chrom'] = cosmic_df['chrom'].astype(np.int8)\n",
    "cosmic_df['pos'] = cosmic_df['pos'].astype(np.uint32)\n",
    "cosmic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d5bbc4-59a8-4045-806e-767d529c4dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>pos</th>\n",
       "      <th>id</th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>maf</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>51479</td>\n",
       "      <td>rs116400033</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>0.104199</td>\n",
       "      <td>common</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>51954</td>\n",
       "      <td>rs185832753</td>\n",
       "      <td>G</td>\n",
       "      <td>A,C,T</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>rare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>54490</td>\n",
       "      <td>rs141149254</td>\n",
       "      <td>G</td>\n",
       "      <td>A,C</td>\n",
       "      <td>0.096939</td>\n",
       "      <td>common</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>54669</td>\n",
       "      <td>rs532505601</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>rare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>54716</td>\n",
       "      <td>rs569128616</td>\n",
       "      <td>C</td>\n",
       "      <td>A,T</td>\n",
       "      <td>0.213305</td>\n",
       "      <td>common</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69542010</th>\n",
       "      <td>22</td>\n",
       "      <td>50795771</td>\n",
       "      <td>rs6010092</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>0.032182</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69542011</th>\n",
       "      <td>22</td>\n",
       "      <td>50795915</td>\n",
       "      <td>rs374867791</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>0.076727</td>\n",
       "      <td>common</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69542012</th>\n",
       "      <td>22</td>\n",
       "      <td>50796090</td>\n",
       "      <td>rs537052521</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>rare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69542013</th>\n",
       "      <td>22</td>\n",
       "      <td>50796204</td>\n",
       "      <td>rs6010093</td>\n",
       "      <td>G</td>\n",
       "      <td>A,C</td>\n",
       "      <td>0.002943</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69542014</th>\n",
       "      <td>22</td>\n",
       "      <td>50799907</td>\n",
       "      <td>rs541448644</td>\n",
       "      <td>A</td>\n",
       "      <td>G,T</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>rare</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69542015 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          chrom       pos           id ref    alt       maf category\n",
       "0             1     51479  rs116400033   T      A  0.104199   common\n",
       "1             1     51954  rs185832753   G  A,C,T  0.000196     rare\n",
       "2             1     54490  rs141149254   G    A,C  0.096939   common\n",
       "3             1     54669  rs532505601   C      T  0.000196     rare\n",
       "4             1     54716  rs569128616   C    A,T  0.213305   common\n",
       "...         ...       ...          ...  ..    ...       ...      ...\n",
       "69542010     22  50795771    rs6010092   T      C  0.032182     none\n",
       "69542011     22  50795915  rs374867791   G      T  0.076727   common\n",
       "69542012     22  50796090  rs537052521   C      G  0.000196     rare\n",
       "69542013     22  50796204    rs6010093   G    A,C  0.002943     none\n",
       "69542014     22  50799907  rs541448644   A    G,T  0.000196     rare\n",
       "\n",
       "[69542015 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inPath = '/scratch/ml-csm/datasets/genomics/ref-genome/human/GRCh38/ensembl/variants/processed/'\n",
    "df_maf = pd.read_parquet(inPath+'1000GENOMES-release114-maf.parquet.gz')\n",
    "df_maf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ba2b3d2-d015-4d64-a4d3-78ab8af8936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 69542015 entries, 0 to 69542014\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Dtype  \n",
      "---  ------    -----  \n",
      " 0   chrom     int8   \n",
      " 1   pos       uint32 \n",
      " 2   id        object \n",
      " 3   ref       object \n",
      " 4   alt       object \n",
      " 5   maf       float32\n",
      " 6   category  object \n",
      "dtypes: float32(1), int8(1), object(4), uint32(1)\n",
      "memory usage: 2.7+ GB\n"
     ]
    }
   ],
   "source": [
    "df_maf['chrom'] = df_maf['chrom'].astype(np.int8)\n",
    "df_maf['pos'] = df_maf['pos'].astype(np.uint32)\n",
    "df_maf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f34833b0-788b-4757-8112-0a71d868be1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe sizes:\n",
      "ClinVar: 3,099,330 rows\n",
      "COSMIC: 16,727,030 rows\n",
      "MAF: 69,542,015 rows\n",
      "Total rows before deduplication: 89,368,375\n",
      "Combined dataframe: 77,304,080 unique rows with 2 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>66926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>69134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>69314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>69423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>69581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chrom    pos\n",
       "0      1  66926\n",
       "1      1  69134\n",
       "2      1  69314\n",
       "3      1  69423\n",
       "4      1  69581"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all three dataframes, keeping only chrom and pos columns\n",
    "\n",
    "# Extract only the relevant columns from each dataframe\n",
    "cln_var_subset = cln_var_df[['chrom', 'pos']]\n",
    "cosmic_subset = cosmic_df[['chrom', 'pos']]\n",
    "df_maf_subset = df_maf[['chrom', 'pos']]\n",
    "\n",
    "# Concatenate all dataframes\n",
    "combined_df = pd.concat([cln_var_subset, cosmic_subset, df_maf_subset], axis=0)\n",
    "\n",
    "# Remove duplicates\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "# Reset index\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "# Add a dummy 'label' column with zeros\n",
    "# combined_df['label'] = 0\n",
    "\n",
    "# Display information about the combined dataframe\n",
    "print(f\"Original dataframe sizes:\")\n",
    "print(f\"ClinVar: {cln_var_subset.shape[0]:,} rows\")\n",
    "print(f\"COSMIC: {cosmic_subset.shape[0]:,} rows\")\n",
    "print(f\"MAF: {df_maf_subset.shape[0]:,} rows\")\n",
    "print(f\"Total rows before deduplication: {cln_var_subset.shape[0] + cosmic_subset.shape[0] + df_maf_subset.shape[0]:,}\")\n",
    "print(f\"Combined dataframe: {combined_df.shape[0]:,} unique rows with {combined_df.shape[1]} columns\")\n",
    "\n",
    "# Display the first few rows of the combined dataframe\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87b827aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of combined dataframe: 368.61 MB\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "# combined_df.info()\n",
    "\n",
    "# Ensure data types are consistent\n",
    "combined_df['chrom'] = combined_df['chrom'].astype(np.int8)\n",
    "combined_df['pos'] = combined_df['pos'].astype(np.uint32)\n",
    "# combined_df['label'] = combined_df['label'].astype(np.int8)  # Use int8 for the label column to save memory\n",
    "\n",
    "# # Save the combined dataframe to parquet format\n",
    "# output_path = '/scratch/ml-csm/projects/fgenom/gve/data/combined_variant_positions.parquet.gz'\n",
    "# combined_df.to_parquet(output_path, compression='gzip')\n",
    "# print(f\"Combined variant positions saved to: {output_path}\")\n",
    "\n",
    "# Memory usage information\n",
    "memory_usage = combined_df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "print(f\"Memory usage of combined dataframe: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05c42412-4236-4bdd-a752-ce43190d1696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77304080, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = '/scratch/ml-csm/projects/fgenom/gve/data/combined_variant_positions.tsv'\n",
    "combined_df.to_csv(output_path, sep='\\t', header=None, index=False)\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e930bc5-cfc0-42d3-aadc-fb0889a01fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chromosome var vcf files: 22\n"
     ]
    }
   ],
   "source": [
    "chr_vcf_folder = '/scratch/ml-csm/projects/fgenom/gve/data/human/ensembl/GRCh38/variant/processed/chroms/'\n",
    "vcf_files = os.listdir(chr_vcf_folder)\n",
    "\n",
    "vcf_files.remove('homo_sapiens-chrX.tsv')\n",
    "vcf_files.remove('homo_sapiens-chrY.tsv')\n",
    "vcf_files.sort()\n",
    "print(f'chromosome var vcf files: {len(vcf_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcbfb3b5-6a7b-4997-b2ab-8dc54ce9b8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original VCF file shape: (54950388, 6)\n",
      "homo_sapiens-chr1.tsv: filtered to (6335460, 6) variants based on position matching\n",
      "Original VCF file shape: (32448977, 6)\n",
      "homo_sapiens-chr10.tsv: filtered to (3866487, 6) variants based on position matching\n",
      "Original VCF file shape: (33255335, 6)\n",
      "homo_sapiens-chr11.tsv: filtered to (4023851, 6) variants based on position matching\n",
      "Original VCF file shape: (32146515, 6)\n",
      "homo_sapiens-chr12.tsv: filtered to (3752760, 6) variants based on position matching\n",
      "Original VCF file shape: (23665483, 6)\n",
      "homo_sapiens-chr13.tsv: filtered to (2811679, 6) variants based on position matching\n",
      "Original VCF file shape: (21621402, 6)\n",
      "homo_sapiens-chr14.tsv: filtered to (2624409, 6) variants based on position matching\n",
      "Original VCF file shape: (20235594, 6)\n",
      "homo_sapiens-chr15.tsv: filtered to (2363241, 6) variants based on position matching\n",
      "Original VCF file shape: (22226709, 6)\n",
      "homo_sapiens-chr16.tsv: filtered to (2675267, 6) variants based on position matching\n",
      "Original VCF file shape: (19772918, 6)\n",
      "homo_sapiens-chr17.tsv: filtered to (2345247, 6) variants based on position matching\n",
      "Original VCF file shape: (18736706, 6)\n",
      "homo_sapiens-chr18.tsv: filtered to (2220182, 6) variants based on position matching\n",
      "Original VCF file shape: (15201974, 6)\n",
      "homo_sapiens-chr19.tsv: filtered to (1951964, 6) variants based on position matching\n",
      "Original VCF file shape: (58802274, 6)\n",
      "homo_sapiens-chr2.tsv: filtered to (6955005, 6) variants based on position matching\n",
      "Original VCF file shape: (15418796, 6)\n",
      "homo_sapiens-chr20.tsv: filtered to (1811824, 6) variants based on position matching\n",
      "Original VCF file shape: (9243118, 6)\n",
      "homo_sapiens-chr21.tsv: filtered to (1059587, 6) variants based on position matching\n",
      "Original VCF file shape: (9622809, 6)\n",
      "homo_sapiens-chr22.tsv: filtered to (1092282, 6) variants based on position matching\n",
      "Original VCF file shape: (48098212, 6)\n",
      "homo_sapiens-chr3.tsv: filtered to (5696278, 6) variants based on position matching\n",
      "Original VCF file shape: (46221323, 6)\n",
      "homo_sapiens-chr4.tsv: filtered to (5568752, 6) variants based on position matching\n",
      "Original VCF file shape: (43338895, 6)\n",
      "homo_sapiens-chr5.tsv: filtered to (5182001, 6) variants based on position matching\n",
      "Original VCF file shape: (40569160, 6)\n",
      "homo_sapiens-chr6.tsv: filtered to (4897794, 6) variants based on position matching\n",
      "Original VCF file shape: (38921175, 6)\n",
      "homo_sapiens-chr7.tsv: filtered to (4579108, 6) variants based on position matching\n",
      "Original VCF file shape: (36818232, 6)\n",
      "homo_sapiens-chr8.tsv: filtered to (4481593, 6) variants based on position matching\n",
      "Original VCF file shape: (30548188, 6)\n",
      "homo_sapiens-chr9.tsv: filtered to (3441869, 6) variants based on position matching\n",
      "Final top_vcf after removing duplicates: (79736640, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>STRAND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>10108</td>\n",
       "      <td>rs62651026</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1</td>\n",
       "      <td>10151</td>\n",
       "      <td>rs1570391830</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>10175</td>\n",
       "      <td>rs1557426757</td>\n",
       "      <td>TAACC</td>\n",
       "      <td>T</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>10179</td>\n",
       "      <td>rs1312716213</td>\n",
       "      <td>CTAA</td>\n",
       "      <td>C</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>10179</td>\n",
       "      <td>rs1557426763</td>\n",
       "      <td>CTAAC</td>\n",
       "      <td>C</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    #CHROM    POS            ID    REF ALT STRAND\n",
       "34       1  10108    rs62651026      C   T      .\n",
       "63       1  10151  rs1570391830      T   A      .\n",
       "86       1  10175  rs1557426757  TAACC   T      .\n",
       "90       1  10179  rs1312716213   CTAA   C      .\n",
       "91       1  10179  rs1557426763  CTAAC   C      ."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_vcf = pd.DataFrame()\n",
    "\n",
    "for vcf_file in vcf_files:\n",
    "    # Read the VCF file\n",
    "    vcf = pd.read_csv(chr_vcf_folder+vcf_file, sep='\\t')\n",
    "    print(f'Original VCF file shape: {vcf.shape}')\n",
    "    \n",
    "    # Extract chromosome number from filename\n",
    "    chrom = int(re.search(r'chr(\\d+)', vcf_file).group(1))\n",
    "    \n",
    "    # Convert VCF column types to match combined_df for proper comparison\n",
    "    vcf['#CHROM'] = vcf['#CHROM'].astype(np.int8)\n",
    "    vcf['POS'] = vcf['POS'].astype(np.uint32)\n",
    "    \n",
    "    # Filter VCF based on chromosome and position from combined_df\n",
    "    # First get positions from combined_df for the current chromosome\n",
    "    chrom_positions = combined_df[combined_df['chrom'] == chrom]['pos'].values\n",
    "    \n",
    "    # Then filter VCF to only include rows with positions in chrom_positions\n",
    "    chr_top_vcf = vcf[vcf['POS'].isin(chrom_positions)]\n",
    "    \n",
    "    # Concatenate with existing top_vcf dataframe\n",
    "    top_vcf = pd.concat([top_vcf, chr_top_vcf])\n",
    "    \n",
    "    print(f'{vcf_file}: filtered to {chr_top_vcf.shape} variants based on position matching')\n",
    "\n",
    "# Remove duplicates based on variant ID\n",
    "dup_index = top_vcf.ID.duplicated()\n",
    "top_vcf = top_vcf[~dup_index]\n",
    "print(f'Final top_vcf after removing duplicates: {top_vcf.shape}')\n",
    "\n",
    "# Display the first few rows of the filtered VCF data\n",
    "top_vcf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f29953d-5135-40f8-9341-eaa1c28f20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_vcf.to_csv('/scratch/ml-csm/projects/fgenom/gve/data/comb_clinVar_COSMIC_MAF.tsv', sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4671fc39-7945-4819-9957-6b7a9bcd0581",
   "metadata": {},
   "source": [
    "# Create Chunks of Variants\n",
    "\n",
    "When dealing with large datasets, it can be beneficial to break them down into smaller, more manageable chunks. This section splits the combined variant dataset into chunks of 10 million rows each and saves them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f862ed-b0f8-4b24-8506-6682fab9be64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 79,736,640 rows and 6 columns\n",
      "   0      1             2      3  4  5\n",
      "0  1  10108    rs62651026      C  T  .\n",
      "1  1  10151  rs1570391830      T  A  .\n",
      "2  1  10175  rs1557426757  TAACC  T  .\n",
      "3  1  10179  rs1312716213   CTAA  C  .\n",
      "4  1  10179  rs1557426763  CTAAC  C  .\n"
     ]
    }
   ],
   "source": [
    "# Load the combined variant dataset\n",
    "df = pd.read_csv('/scratch/ml-csm/projects/fgenom/gve/data/comb_clinVar_COSMIC_MAF.tsv', sep='\\t', header=None)\n",
    "print(f\"Loaded dataset with {df.shape[0]:,} rows and {df.shape[1]} columns\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d56664f-d428-4fd1-94cf-36defcfb6634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory for chunks: /scratch/ml-csm/projects/fgenom/gve/data/comb_chunks\n",
      "Will create 8 chunks of approximately 10,000,000 rows each\n",
      "Saved chunk 1/8: /scratch/ml-csm/projects/fgenom/gve/data/comb_chunks/variant_chunk_001.tsv with 10,000,000 rows\n",
      "Saved chunk 2/8: /scratch/ml-csm/projects/fgenom/gve/data/comb_chunks/variant_chunk_002.tsv with 10,000,000 rows\n",
      "Saved chunk 3/8: /scratch/ml-csm/projects/fgenom/gve/data/comb_chunks/variant_chunk_003.tsv with 10,000,000 rows\n",
      "Saved chunk 4/8: /scratch/ml-csm/projects/fgenom/gve/data/comb_chunks/variant_chunk_004.tsv with 10,000,000 rows\n",
      "Saved chunk 5/8: /scratch/ml-csm/projects/fgenom/gve/data/comb_chunks/variant_chunk_005.tsv with 10,000,000 rows\n",
      "Saved chunk 6/8: /scratch/ml-csm/projects/fgenom/gve/data/comb_chunks/variant_chunk_006.tsv with 10,000,000 rows\n",
      "Saved chunk 7/8: /scratch/ml-csm/projects/fgenom/gve/data/comb_chunks/variant_chunk_007.tsv with 10,000,000 rows\n",
      "Saved chunk 8/8: /scratch/ml-csm/projects/fgenom/gve/data/comb_chunks/variant_chunk_008.tsv with 9,736,640 rows\n",
      "\n",
      "Completed: Created 8 chunks in /scratch/ml-csm/projects/fgenom/gve/data/comb_chunks\n"
     ]
    }
   ],
   "source": [
    "# Create directory for chunks if it doesn't exist\n",
    "chunk_dir = '/scratch/ml-csm/projects/fgenom/gve/data/comb_chunks'\n",
    "os.makedirs(chunk_dir, exist_ok=True)\n",
    "print(f\"Created directory for chunks: {chunk_dir}\")\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10_000_000\n",
    "\n",
    "# Calculate number of chunks needed\n",
    "n_chunks = (df.shape[0] + chunk_size - 1) // chunk_size  # ceiling division\n",
    "print(f\"Will create {n_chunks} chunks of approximately {chunk_size:,} rows each\")\n",
    "\n",
    "# Create and save chunks\n",
    "for i in range(n_chunks):\n",
    "    # Calculate start and end indices for this chunk\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, df.shape[0])\n",
    "    \n",
    "    # Extract chunk\n",
    "    chunk = df.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Create chunk filename\n",
    "    chunk_file = os.path.join(chunk_dir, f'variant_chunk_{i+1:03d}.tsv')\n",
    "    \n",
    "    # Save chunk\n",
    "    chunk.to_csv(chunk_file, sep='\\t', header=None, index=False)\n",
    "    \n",
    "    print(f\"Saved chunk {i+1}/{n_chunks}: {chunk_file} with {len(chunk):,} rows\")\n",
    "\n",
    "print(f\"\\nCompleted: Created {n_chunks} chunks in {chunk_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7eddc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 8 chunk files:\n",
      "  variant_chunk_001.tsv: 289.49 MB\n",
      "  variant_chunk_002.tsv: 293.78 MB\n",
      "  variant_chunk_003.tsv: 292.41 MB\n",
      "  variant_chunk_004.tsv: 288.66 MB\n",
      "  variant_chunk_005.tsv: 288.50 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntotal_rows = 0\\nfor chunk_file in chunk_files:\\n    file_path = os.path.join(chunk_dir, chunk_file)\\n    chunk_df = pd.read_csv(file_path, sep=\\'\\t\\', header=None)\\n    total_rows += len(chunk_df)\\n\\nprint(f\"\\nTotal rows in all chunks: {total_rows:,}\")\\nprint(f\"Original dataframe rows: {df.shape[0]:,}\")\\nprint(f\"Rows match: {total_rows == df.shape[0]}\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify chunks (optional)\n",
    "# List all files in the chunk directory\n",
    "chunk_files = sorted(os.listdir(chunk_dir))\n",
    "print(f\"Created {len(chunk_files)} chunk files:\")\n",
    "\n",
    "# Check total size of all chunks\n",
    "total_rows = 0\n",
    "for chunk_file in chunk_files[:5]:  # Show only first 5 for brevity\n",
    "    file_path = os.path.join(chunk_dir, chunk_file)\n",
    "    file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "    print(f\"  {chunk_file}: {file_size:.2f} MB\")\n",
    "\n",
    "# You can uncomment the following code to verify the total number of rows\n",
    "# across all chunks matches the original dataframe\n",
    "'''\n",
    "total_rows = 0\n",
    "for chunk_file in chunk_files:\n",
    "    file_path = os.path.join(chunk_dir, chunk_file)\n",
    "    chunk_df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    total_rows += len(chunk_df)\n",
    "\n",
    "print(f\"\\nTotal rows in all chunks: {total_rows:,}\")\n",
    "print(f\"Original dataframe rows: {df.shape[0]:,}\")\n",
    "print(f\"Rows match: {total_rows == df.shape[0]}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374c0a79",
   "metadata": {},
   "source": [
    "## Chunking Summary\n",
    "\n",
    "Breaking down the large variant dataset into smaller chunks offers several advantages:\n",
    "\n",
    "1. **Memory Efficiency**: Processing smaller chunks requires less memory, allowing analysis on machines with limited resources.\n",
    "2. **Parallel Processing**: Chunks can be processed independently, enabling parallel computation.\n",
    "3. **Fault Tolerance**: If processing fails for one chunk, others can still be processed.\n",
    "4. **Storage Flexibility**: Smaller files can be more easily transferred and stored.\n",
    "\n",
    "These chunks can now be used for downstream analysis, with each chunk containing approximately 10 million variants."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
